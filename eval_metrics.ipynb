{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/r1-reasoning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataset import ReasoningHashDataset\n",
    "checkpoint_path = \"model_20250206_083544/model_checkpoint_batch_300\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
    "dataset = ReasoningHashDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=10000,  # Small number for testing\n",
    "    hash_length=4,  # Shorter hashes for testing\n",
    "    chains=[2, 3, 4, 5],  # Simpler chain lengths\n",
    "    vary_hash=True,\n",
    "    num_chains=3,\n",
    "    device=\"cuda\",\n",
    "    rl=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test output: Map:\n",
      "10qy=>1vvn\n",
      "1vvn=>nyw5\n",
      "j34i=>9ru9\n",
      "hdka=>fk9r\n",
      "w0dn=>10qy\n",
      "2rts=>w0dn\n",
      "k0l1=>3c5x\n",
      "2rts=>ev3v\n",
      "9ru9=>hdka\n",
      "2rts=>j34i\n",
      "ev3v=>k0l1\n",
      "Start: 2rts\n",
      "Task: Multiple hash chains are provided. Find the shortest chain and provide the end 4 char hash. Think hard in tag! Circle your answer in <circle>HERE</circle> after </think>\n",
      "-----\n",
      "START\n",
      "<think>There are two lists provided. 2rts is listed twice, once as ev3v and once as w0dn. 1vvn is listed twice, once as 10qy and once as nyw5. 9ru9 is listed twice, once as j34i and once as 9ru9. 10qy is listed once as 1vvn and once as 10qy. 2rts is listed once as ev3v and once as w0dn. hdka is listed once as hdka and once as fk9r. k0l1 is listed once as k0l1 and once as 3c5x. j34i is listed once as j34i and once as 9ru9. w0dn is listed once as w0dn and once as 10qy. nyw5 is listed once as 1vvn and once as nyw5. 2rts is listed once as 2rts and once as ev3v. 3c5x is listed once as k0l1 and once as 3c5x. 9ru9 is listed once as 9ru9 and once as hdka. 1vvn is listed once as 1vvn and once as nyw5. 10qy is listed once as 10qy and once as 1vvn. ev3v is listed once as ev3v and once as 2rts. hdka is listed once as hdka and once as fk9r. \n",
      "</think>\n",
      "<circle>ev3v</circle>\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "\n",
    "# Load tokenizer and model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=T.bfloat16\n",
    ")\n",
    "\n",
    "# Set model to eval mode for inference\n",
    "model.eval()\n",
    "\n",
    "# Quick test to verify loading\n",
    "# test_input = \"Map:\\n9lqz=>9h1e\\nmvnz=>z0e6\\nmvnz=>9lqz\\nz0e6=>xtka\\nmvnz=>7ck5\\nxtka=>ati4\\nStart: mvnz\\nTask: Multiple hash chains are provided. Find the shortest chain and provide the end 4 char hash. Think hard in tag! Circle your answer in <circle>HERE</circle> after </think>\\n-----\\nSTART\\n\"\n",
    "# input_ids = tokenizer(t, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with T.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=dataset[0][\"input\"][\"input_ids\"].unsqueeze(0),\n",
    "        attention_mask=dataset[0][\"input\"][\"attention_mask\"].unsqueeze(0),\n",
    "        max_new_tokens=4000,\n",
    "        temperature=0.85,\n",
    "        top_p=0.8,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(\"Test output:\", tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def generate_hash_consensus(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    num_sequences: int = 8,\n",
    "    max_length: int = 400\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate multiple hashes and return the most common one.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(model.device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        num_return_sequences=num_sequences,\n",
    "        temperature=0.8,\n",
    "        top_p=0.85,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_length\n",
    "    )\n",
    "    \n",
    "    hashes = []\n",
    "    for sequence in output:\n",
    "        generated_text = tokenizer.decode(sequence)\n",
    "        generated_text = generated_text[generated_text.index(prompt)+len(prompt):]\n",
    "        \n",
    "        try:\n",
    "            if \"<circle>\" in generated_text and \"</circle>\" in generated_text:\n",
    "                circle = generated_text.split(\"<circle>\")[1].split(\"</circle>\")[0]\n",
    "                hashes.append(circle)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not hashes:\n",
    "        return \"NO CIRCLE\"\n",
    "    return hashes\n",
    "\n",
    "def generate_hash(model, tokenizer, prompt, max_length=100):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(model.device)\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=pad_token_id,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0])\n",
    "    # slide to just generated part\n",
    "    generated_text = generated_text[generated_text.index(prompt)+len(prompt):]\n",
    "    # get whats in between <circle> and </circle>\n",
    "    # check if it has circle\n",
    "    try:\n",
    "        if \"<circle>\" not in generated_text and \"</circle>\" not in generated_text:\n",
    "            return \"NO CIRCLE\"\n",
    "        circle = generated_text.split(\"<circle>\")[1].split(\"</circle>\")[0]\n",
    "    except:\n",
    "        return \"NO CIRCLE\"\n",
    "    return circle\n",
    "\n",
    "def evaluate_reasoning_hash(model, tokenizer, num_tests: int = 100, hash_length: int = 5, \n",
    "                            chains: List[int] = [3, 4, 5, 6], vary_hash: bool = True, num_chains: int = 4):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create evaluation dataset\n",
    "    eval_dataset = ReasoningHashDataset(tokenizer, num_samples=num_tests, hash_length=hash_length, \n",
    "                                        chains=chains, vary_hash=vary_hash, num_chains=num_chains)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    chain_accuracies = {}\n",
    "\n",
    "    for _ in tqdm(range(len(eval_dataset)), desc=\"Evaluating Reasoning Hash\", leave=False):\n",
    "        full_text, hash_list, start, actual_target, prompt = eval_dataset.get_eval_item(_)\n",
    "        predicted_target = generate_hash_consensus(model, tokenizer, prompt)\n",
    "        if actual_target in predicted_target:\n",
    "            correct_predictions += 1\n",
    "        # predicted_target = predicted_target[:3].lower()\n",
    "        # actual_target = actual_target[:3].lower()\n",
    "\n",
    "        total_predictions += 1\n",
    "        # if any(predicted_target[i:i+3] == actual_target[i:i+3] for i in range(len(predicted_target)-2)):\n",
    "        #     correct_predictions += 1\n",
    "        \n",
    "        # Determine the chain length for this sample\n",
    "        chain_length = ReasoningHashDataset.find_shortest_path(hash_list, start) + 1\n",
    "        \n",
    "        # Initialize the chain_length entry if it doesn't exist\n",
    "        if chain_length not in chain_accuracies:\n",
    "            chain_accuracies[chain_length] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        chain_accuracies[chain_length]['total'] += 1\n",
    "        if  actual_target in predicted_target:\n",
    "            chain_accuracies[chain_length]['correct'] += 1\n",
    "        else:\n",
    "            print(f\"@@@Failed with {predicted_target} and {actual_target}\")\n",
    "\n",
    "    overall_accuracy = correct_predictions / total_predictions\n",
    "    for length in chain_accuracies:\n",
    "        if chain_accuracies[length]['total'] > 0:\n",
    "            chain_accuracies[length]['accuracy'] = chain_accuracies[length]['correct'] / chain_accuracies[length]['total']\n",
    "        else:\n",
    "            chain_accuracies[length]['accuracy'] = 0\n",
    "\n",
    "    return overall_accuracy, chain_accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
